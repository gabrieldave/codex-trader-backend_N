"""
üìä GENERAR REPORTE FINAL DE INGESTA
====================================

Genera un reporte final completo de la ingesta.
"""

import os
import sys
import psycopg2
from datetime import datetime
from urllib.parse import quote_plus
from dotenv import load_dotenv

if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

load_dotenv()

def get_env(key):
    value = os.getenv(key, "")
    if not value:
        for env_key in os.environ.keys():
            if env_key.strip().lstrip('\ufeff') == key:
                value = os.environ[env_key]
                break
    return value.strip('"').strip("'").strip()

SUPABASE_URL = get_env("SUPABASE_URL")
SUPABASE_DB_PASSWORD = get_env("SUPABASE_DB_PASSWORD")

if not SUPABASE_URL or not SUPABASE_DB_PASSWORD:
    print("‚ö†Ô∏è  Faltan variables de entorno")
    sys.exit(1)

project_ref = SUPABASE_URL.replace("https://", "").replace(".supabase.co", "")
encoded_password = quote_plus(SUPABASE_DB_PASSWORD)
postgres_connection_string = f"postgresql://postgres:{encoded_password}@db.{project_ref}.supabase.co:5432/postgres"

try:
    import config
    collection_name = config.VECTOR_COLLECTION_NAME
except ImportError:
    collection_name = "knowledge"

def get_final_stats():
    """Obtiene estad√≠sticas finales"""
    stats = {
        'chunks': None,
        'files_estimated': None,
        'db_size': None,
        'errors': None
    }
    
    try:
        conn = psycopg2.connect(postgres_connection_string, connect_timeout=20)
        cur = conn.cursor()
        cur.execute("SET statement_timeout = '30s'")
        
        # Chunks
        try:
            cur.execute("""
                SELECT n_live_tup
                FROM pg_stat_user_tables
                WHERE schemaname = 'vecs' AND relname = %s
            """, (collection_name,))
            result = cur.fetchone()
            if result and result[0]:
                stats['chunks'] = result[0]
                stats['files_estimated'] = result[0] // 100
        except:
            pass
        
        # Tama√±o de BD
        try:
            cur.execute("""
                SELECT 
                    pg_size_pretty(pg_database_size(current_database())) as db_size,
                    pg_database_size(current_database()) as db_size_bytes
            """)
            result = cur.fetchone()
            if result:
                stats['db_size'] = result[0]
                stats['db_size_bytes'] = result[1]
        except:
            pass
        
        # Errores
        try:
            cur.execute("""
                SELECT COUNT(*) as count
                FROM ingestion_errors
            """)
            result = cur.fetchone()
            if result:
                stats['errors'] = result[0]
        except:
            pass
        
        cur.close()
        conn.close()
    except Exception as e:
        print(f"‚ö†Ô∏è  Error obteniendo estad√≠sticas: {e}")
    
    return stats

def generar_reporte():
    """Genera el reporte final"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"REPORTE_FINAL_INGESTA_{timestamp}.md"
    
    stats = get_final_stats()
    
    reporte = f"""# üìä REPORTE FINAL DE INGESTA RAG

**Fecha de generaci√≥n**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ‚úÖ RESUMEN EJECUTIVO

La ingesta de documentos ha **completado exitosamente**.

---

## üìä ESTAD√çSTICAS FINALES

| M√©trica | Valor |
|---------|-------|
| **Chunks indexados** | {stats['chunks']:,} |
| **Archivos estimados** | ~{stats['files_estimated']:,} |
| **Tama√±o de base de datos** | {stats['db_size'] or 'N/A'} |
"""
    
    if stats['errors'] is not None:
        reporte += f"| **Errores registrados** | {stats['errors']} |\n"
    
    reporte += f"""
---

## üìà DISTRIBUCI√ìN DE DATOS

- **Chunks por archivo (promedio)**: ~100 chunks
- **Tama√±o promedio por chunk**: ~1,024 caracteres
- **Total de caracteres indexados**: ~{stats['chunks'] * 1024:,} caracteres

---

## üîß CONFIGURACI√ìN UTILIZADA

- **Workers**: 5 (configuraci√≥n reducida)
- **Batch size**: 20 chunks por request
- **Chunk size**: 1,024 caracteres
- **Chunk overlap**: 200 caracteres
- **Modelo de embeddings**: text-embedding-3-small (1536 dimensiones)

---

## ‚ö†Ô∏è NOTAS IMPORTANTES

1. **Configuraci√≥n reducida aplicada**: Se redujeron los workers de 15 a 5 para evitar sobrecarga en Supabase
2. **CPU Supabase**: Se detect√≥ CPU al 100% durante la ingesta, por lo que se aplic√≥ configuraci√≥n reducida
3. **Proceso √∫nico**: Se ejecut√≥ 1 solo proceso (no 3 en paralelo) para reducir carga

---

## üìù PR√ìXIMOS PASOS RECOMENDADOS

1. ‚úÖ Verificar que Supabase est√© estable (CPU, Memory, IOPS)
2. ‚úÖ Probar b√∫squedas RAG con los documentos indexados
3. ‚úÖ Revisar archivos sospechosos (si los hay) en el reporte detallado
4. ‚úÖ Considerar optimizaciones futuras si es necesario

---

## üéâ CONCLUSI√ìN

La ingesta se complet√≥ exitosamente con **{stats['chunks']:,} chunks** indexados, representando aproximadamente **{stats['files_estimated']:,} archivos**.

El sistema est√° listo para realizar b√∫squedas RAG sobre el contenido indexado.

---

*Reporte generado autom√°ticamente el {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    # Guardar reporte
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(reporte)
    
    print("="*80)
    print("üìä REPORTE FINAL GENERADO")
    print("="*80)
    print()
    print(f"‚úÖ Archivo: {filename}")
    print()
    print("üìã Resumen:")
    print(f"   üì¶ Chunks: {stats['chunks']:,}")
    print(f"   üìö Archivos: ~{stats['files_estimated']:,}")
    print(f"   üíæ Tama√±o BD: {stats['db_size'] or 'N/A'}")
    if stats['errors'] is not None:
        print(f"   ‚ö†Ô∏è  Errores: {stats['errors']}")
    print()
    print("="*80)
    
    # Mostrar reporte en consola tambi√©n
    print()
    print(reporte)
    
    return filename

if __name__ == "__main__":
    generar_reporte()















