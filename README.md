# üìö Sistema RAG de Ingesta de Libros

## 1. Introducci√≥n General

### ¬øQu√© es este proyecto?

Este proyecto implementa un sistema completo de **RAG (Retrieval-Augmented Generation)** para procesar, indexar y consultar documentos (principalmente libros en formato PDF, EPUB, TXT, DOCX, MD) usando embeddings vectoriales y b√∫squeda sem√°ntica.

### ¬øQu√© problema resuelve?

El sistema resuelve el problema de **indexar grandes colecciones de libros/documentos** y permitir consultas sem√°nticas sobre su contenido:

- **Ingesta masiva**: Procesa cientos o miles de archivos de forma eficiente y paralela
- **B√∫squeda sem√°ntica**: Permite hacer preguntas en lenguaje natural sobre el contenido indexado
- **Anti-duplicados**: Evita indexar el mismo contenido dos veces
- **Control de l√≠mites**: Respeta los l√≠mites de la API de OpenAI (Tier 3) sin excederlos
- **Monitoreo en tiempo real**: Muestra progreso, velocidad y m√©tricas durante la ingesta
- **Reporte detallado**: Genera un reporte final con estad√≠sticas completas

### Tecnolog√≠as Principales

- **Python 3.x** - Lenguaje principal
- **LlamaIndex** - Framework para RAG y procesamiento de documentos
- **SentenceTransformer** - Embeddings locales con modelo `all-MiniLM-L6-v2` (384 dimensiones)
- **Supabase** - Base de datos PostgreSQL con extensi√≥n pgvector para almacenamiento vectorial
- **FastAPI** - API REST para consultas RAG
- **LiteLLM** - Abstracci√≥n para usar m√∫ltiples modelos de IA (OpenAI, Deepseek, Claude, Gemini, etc.)

---

## 2. Arquitectura General del Sistema

### Flujo End-to-End

El sistema funciona en dos fases principales:

#### **Fase 1: Ingesta de Documentos**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE DE INGESTI√ìN                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1Ô∏è‚É£ LEER ARCHIVOS
   ‚Üì
   SimpleDirectoryReader(input_files=[file_path])
   ‚Ä¢ Soporta: PDF, EPUB, TXT, DOCX, MD
   ‚Ä¢ Lee desde ./data/
   ‚Ä¢ Convierte autom√°ticamente a texto

2Ô∏è‚É£ VERIFICAR DUPLICADOS
   ‚Üì
   calculate_doc_id(file_path)  # Hash SHA256
   ‚Ä¢ Consulta tabla documents en Supabase
   ‚Ä¢ Si existe ‚Üí SKIP o REINDEX (seg√∫n configuraci√≥n)
   ‚Ä¢ Si no existe ‚Üí PROCESS

3Ô∏è‚É£ EXTRAER TEXTO
   ‚Üì
   reader.load_data()
   ‚Ä¢ LlamaIndex extrae texto autom√°ticamente
   ‚Ä¢ Crea objetos Document con metadata

4Ô∏è‚É£ DIVIDIR EN CHUNKS
   ‚Üì
   SentenceSplitter(chunk_size=1024, chunk_overlap=200)
   ‚Ä¢ Divide en chunks de 1024 caracteres
   ‚Ä¢ Overlap de 200 caracteres entre chunks
   ‚Ä¢ Mantiene contexto entre chunks adyacentes

5Ô∏è‚É£ GENERAR EMBEDDINGS
   ‚Üì
   SentenceTransformer('all-MiniLM-L6-v2')
   ‚Ä¢ Ejecuci√≥n local en el servidor
   ‚Ä¢ Genera embeddings de 384 dimensiones
   ‚Ä¢ Sin costo por token (procesamiento local)

6Ô∏è‚É£ GUARDAR EN SUPABASE
   ‚Üì
   SupabaseVectorStore + pgvector
   ‚Ä¢ Almacena vectores en PostgreSQL
   ‚Ä¢ Guarda metadata (file_name, chunk_id, doc_id, etc.)
   ‚Ä¢ Tabla: vecs.knowledge (configurable)
```

#### **Fase 2: Consulta RAG**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE DE CONSULTA                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1Ô∏è‚É£ RECIBIR PREGUNTA
   ‚Üì
   POST /chat { "query": "¬øQu√© dice el libro sobre X?" }

2Ô∏è‚É£ GENERAR EMBEDDING DE LA PREGUNTA
   ‚Üì
   SentenceTransformer('all-MiniLM-L6-v2')
   ‚Ä¢ Ejecuci√≥n local en el servidor
   ‚Ä¢ 384 dimensiones

3Ô∏è‚É£ B√öSQUEDA SEM√ÅNTICA
   ‚Üì
   match_documents_384(query_embedding, match_count)
   ‚Ä¢ Funci√≥n RPC en Supabase
   ‚Ä¢ Busca los chunks m√°s similares usando distancia coseno
   ‚Ä¢ Tabla: book_chunks

4Ô∏è‚É£ CONSTRUIR CONTEXTO
   ‚Üì
   Concatenar chunks recuperados
   ‚Ä¢ Crea contexto para el LLM

5Ô∏è‚É£ GENERAR RESPUESTA
   ‚Üì
   LiteLLM (OpenAI/Deepseek/Claude/Gemini)
   ‚Ä¢ Env√≠a contexto + pregunta al LLM
   ‚Ä¢ Genera respuesta basada en el contexto

6Ô∏è‚É£ DEVOLVER RESPUESTA
   ‚Üì
   { "response": "...", "tokens_usados": 150, "tokens_restantes": 19850 }
```

### Diagrama de Alto Nivel

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Libros/PDFs ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ   Ingesta    ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ   Chunks     ‚îÇ
‚îÇ   ./data/    ‚îÇ     ‚îÇ  (Paralelo)  ‚îÇ     ‚îÇ  (1024 chars)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                   ‚îÇ
                                                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Consulta   ‚îÇ ‚Üê‚îÄ‚îÄ ‚îÇ   Supabase   ‚îÇ ‚Üê‚îÄ‚îÄ ‚îÇ  Embeddings  ‚îÇ
‚îÇ   RAG API    ‚îÇ     ‚îÇ  (pgvector)  ‚îÇ     ‚îÇ  (384 dims)  ‚îÇ
‚îÇ              ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ  (Local)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 3. Detalle de la Ingesta de Documentos

### 3.1. Lectura de Archivos

El sistema usa **LlamaIndex SimpleDirectoryReader** para leer archivos:

```python
reader = SimpleDirectoryReader(input_files=[file_path])
documents = reader.load_data()
```

**Formatos soportados**:
- PDF (`.pdf`)
- EPUB (`.epub`)
- Texto plano (`.txt`)
- Word (`.docx`)
- Markdown (`.md`)

**Ubicaci√≥n**: Los archivos se leen desde `./data/` (configurable en `config.py`)

### 3.2. Extracci√≥n de Texto

LlamaIndex extrae el texto autom√°ticamente seg√∫n el formato:
- **PDF**: Extrae texto de todas las p√°ginas
- **EPUB**: Extrae texto de todos los cap√≠tulos
- **TXT/MD**: Lee el contenido directamente
- **DOCX**: Extrae texto del documento Word

Cada archivo se convierte en uno o m√°s objetos `Document` de LlamaIndex.

### 3.3. Configuraci√≥n de Chunks

El sistema usa **chunking fijo** (no configurable sin solicitud expl√≠cita):

- **Chunk size**: **1024 caracteres** (no tokens)
- **Chunk overlap**: **200 caracteres** (~20% de overlap)
- **Splitter**: `SentenceSplitter` de LlamaIndex

**¬øPor qu√© 1024 caracteres?**
- Equivale a aproximadamente **256 tokens** (1 token ‚âà 4 caracteres)
- Balance entre contexto y precisi√≥n
- Compatible con el modelo de embeddings

**¬øPor qu√© 200 caracteres de overlap?**
- Mantiene contexto entre chunks adyacentes
- Evita cortar frases a la mitad
- Mejora la recuperaci√≥n de informaci√≥n

**C√≥digo**:
```python
text_splitter = SentenceSplitter(
    chunk_size=1024,      # caracteres
    chunk_overlap=200     # caracteres
)
```

### 3.4. Modelo de Embeddings

**Modelo**: `all-MiniLM-L6-v2` de SentenceTransformer (ejecuci√≥n local)

**Caracter√≠sticas**:
- **Dimensiones**: 384
- **Costo**: Sin costo (procesamiento local en el servidor)
- **Calidad**: Excelente para b√∫squeda sem√°ntica
- **Velocidad**: Muy r√°pido (sin latencia de red)
- **Ejecuci√≥n**: Local en el servidor (no requiere API externa)

**¬øPor qu√© este modelo?**
- Sin costo por token (procesamiento local)
- Excelente calidad para b√∫squeda sem√°ntica
- Compatible con pgvector en Supabase
- No depende de l√≠mites de API externa
- Mayor control y privacidad

**C√≥digo**:
```python
from sentence_transformers import SentenceTransformer

local_embedder = SentenceTransformer("all-MiniLM-L6-v2", device="cuda")
# O sin GPU:
local_embedder = SentenceTransformer("all-MiniLM-L6-v2")
```

### 3.5. Generaci√≥n de Embeddings

**Procesamiento**: Local en el servidor (sin API externa)

**Caracter√≠sticas**:
- **Sin l√≠mites de API**: No hay l√≠mites de RPM/TPM para embeddings
- **Procesamiento en batch**: Los chunks se procesan en lotes para eficiencia
- **Sin costo**: No hay costo por token o request
- **Velocidad**: Depende del hardware del servidor (CPU/GPU)

**Ventajas**:
- No requiere API key de OpenAI para embeddings
- Sin preocupaciones por rate limits
- Mayor privacidad (datos no salen del servidor)
- Control total sobre el procesamiento

**C√≥digo**:
```python
from sentence_transformers import SentenceTransformer

# Inicializar embedder local
local_embedder = SentenceTransformer("all-MiniLM-L6-v2", device="cuda")

# Generar embeddings
embeddings = local_embedder.encode(chunks, show_progress_bar=False)
```

### 3.6. Workers y Procesamiento Paralelo

**N√∫mero de workers por defecto**: **15**

**Configuraci√≥n**:
- Configurable mediante variable de entorno: `MAX_WORKERS=15`
- Usa `ThreadPoolExecutor` para concurrencia
- Cada worker procesa un archivo a la vez

**¬øPor qu√© 15 workers?**
- Balance entre velocidad y uso de recursos del servidor
- Optimizado para procesamiento local de embeddings
- No hay l√≠mites de API que respetar (procesamiento local)

**C√≥digo**:
```python
MAX_WORKERS = 15  # configurable
executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)
```

---

## 4. Rendimiento y Procesamiento Local

### 4.1. Procesamiento Local de Embeddings

El sistema usa **embeddings locales** con SentenceTransformer, lo que significa:

- **Sin l√≠mites de API**: No hay l√≠mites de RPM/TPM para embeddings
- **Sin costo por token**: El procesamiento es completamente local
- **Mayor velocidad**: Sin latencia de red para generar embeddings
- **Mayor privacidad**: Los datos no salen del servidor

### 4.2. Rendimiento

**Factores que afectan el rendimiento**:
- **Hardware del servidor**: CPU/GPU disponible
- **N√∫mero de workers**: Paralelizaci√≥n del procesamiento
- **Tama√±o de los chunks**: M√°s chunks = m√°s embeddings a generar
- **Carga del servidor**: Otros procesos ejecut√°ndose

**Optimizaciones**:
- Uso de GPU cuando est√° disponible (`device="cuda"`)
- Procesamiento en batch para eficiencia
- Paralelizaci√≥n con m√∫ltiples workers

### 4.3. Distribuci√≥n del Trabajo

El sistema distribuye el trabajo de la siguiente manera:

1. **M√∫ltiples workers** procesan archivos en paralelo
2. **Cada worker** procesa un archivo completo
3. **Cada archivo** se divide en chunks
4. **Cada batch** de chunks se procesa localmente con SentenceTransformer
5. **Los embeddings** se guardan directamente en Supabase

**Ejemplo**:
- 15 workers procesando en paralelo
- Cada worker procesa archivos seg√∫n capacidad del servidor
- Embeddings generados localmente sin l√≠mites de API
- Sin preocupaciones por rate limits o costos de API

---

## 5. L√≥gica Anti-Duplicados

### 5.1. Detecci√≥n de Duplicados a Nivel de Documento

El sistema usa **hash SHA256 del archivo** para detectar duplicados:

**C√°lculo de `doc_id`**:
```python
doc_id = calculate_doc_id(file_path)  # SHA256 de los bytes del archivo
```

**Ventajas**:
- Detecta duplicados incluso si el archivo tiene diferente nombre
- Detecta contenido id√©ntico en archivos diferentes
- Determin√≠stico: siempre genera el mismo hash para el mismo archivo

**Alternativa disponible**:
```python
doc_id = calculate_doc_id(file_path, use_content_hash=True, content=texto)
```
- Usa hash del contenido normalizado (√∫til para detectar contenido duplicado con formato diferente)

### 5.2. Tabla `documents` en Supabase

**Estructura**:
```sql
CREATE TABLE documents (
    doc_id TEXT PRIMARY KEY,
    filename TEXT NOT NULL,
    file_path TEXT,
    title TEXT,
    hash_method TEXT DEFAULT 'sha256',
    total_chunks INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
)
```

**√çndices**:
- `idx_documents_filename` en `filename`
- `idx_documents_created_at` en `created_at`

**Prop√≥sito**:
- Tracking de documentos indexados
- Verificaci√≥n r√°pida de duplicados
- Auditor√≠a de ingesta

### 5.3. Decisi√≥n de Procesamiento

Antes de procesar un archivo, el sistema toma una decisi√≥n:

```python
action, existing_doc = decide_document_action(doc_id, force_reindex=FORCE_REINDEX)

if action == "skip":
    # Duplicado detectado, saltar
    monitor.on_file_duplicated(file_name, doc_id)
elif action == "reindex":
    # Eliminar chunks anteriores y reindexar
    delete_document_chunks(doc_id, collection_name)
    # Procesar archivo normalmente
elif action == "process":
    # Nuevo documento, procesar normalmente
```

**Configuraci√≥n**:
- **Variable de entorno**: `FORCE_REINDEX=false` (por defecto)
- Si `FORCE_REINDEX=true`: Fuerza reindexaci√≥n incluso si el documento existe

### 5.4. Detecci√≥n de Duplicados a Nivel de Chunk

**C√°lculo de `chunk_id`**:
```python
chunk_id = calculate_chunk_id(doc_id, chunk_index, chunk_content)
# Hash de: doc_id + ":" + chunk_index + ":" + contenido_normalizado
```

**Verificaci√≥n**:
Antes de procesar cada batch, se verifica si el chunk ya existe:

```python
if check_chunk_exists(chunk_id, collection_name):
    # Chunk duplicado, saltar
    continue
```

**Ventajas**:
- Evita duplicar chunks individuales
- √ötil cuando se reindexa un documento parcialmente
- Determ√≠nistico: siempre genera el mismo `chunk_id` para el mismo contenido

### 5.5. Flujo Completo Anti-Duplicados

```
1. Calcular doc_id (hash del archivo)
   ‚Üì
2. Verificar en tabla documents
   ‚Üì
3a. Si existe y FORCE_REINDEX=False ‚Üí SKIP (duplicado)
3b. Si existe y FORCE_REINDEX=True ‚Üí REINDEX (eliminar chunks y procesar)
3c. Si no existe ‚Üí PROCESS (nuevo)
   ‚Üì
4. Procesar archivo (si no es skip)
   ‚Üì
5. Para cada chunk:
   - Calcular chunk_id determin√≠stico
   - Verificar si chunk existe
   - Si existe ‚Üí saltar chunk
   - Si no existe ‚Üí procesar
   ‚Üì
6. Registrar documento en tabla documents
```

---

## 6. Estructura de la Base de Datos en Supabase

### 6.1. Tabla `documents`

**Prop√≥sito**: Tracking de documentos indexados

**Columnas**:
- `doc_id` (TEXT, PRIMARY KEY): Hash SHA256 del archivo
- `filename` (TEXT, NOT NULL): Nombre del archivo
- `file_path` (TEXT): Ruta completa del archivo
- `title` (TEXT): T√≠tulo del documento (opcional)
- `hash_method` (TEXT, DEFAULT 'sha256'): M√©todo de hash usado
- `total_chunks` (INTEGER, DEFAULT 0): N√∫mero total de chunks
- `created_at` (TIMESTAMP, DEFAULT NOW()): Fecha de creaci√≥n
- `updated_at` (TIMESTAMP, DEFAULT NOW()): Fecha de √∫ltima actualizaci√≥n

**√çndices**:
- `idx_documents_filename`: B√∫squeda r√°pida por nombre de archivo
- `idx_documents_created_at`: Ordenamiento por fecha

### 6.2. Tabla `book_chunks` (Almacenamiento de Vectores)

**Prop√≥sito**: Almacenamiento de embeddings y chunks

**Estructura** (pgvector):
- `id` (UUID, PRIMARY KEY): ID √∫nico del chunk
- `embedding` (vector(384)): Embedding vectorial (384 dimensiones)
- `content` (TEXT): Contenido del chunk
- `metadata` (JSONB): Metadatos del chunk

**Funci√≥n RPC de b√∫squeda**: `match_documents_384`
- Recibe: `query_embedding` (vector de 384 dimensiones) y `match_count`
- Retorna: Los chunks m√°s similares usando distancia coseno
- Filtros opcionales: `category_filter` para filtrar por categor√≠a

**Metadatos guardados**:
```json
{
  "file_name": "libro.pdf",
  "chunk_id": "abc123...",
  "doc_id": "def456...",
  "chunk_index": 0,
  "total_chunks": 100,
  "char_range": "0-1024",
  "book_title": "T√≠tulo del Libro"
}
```

**Relaciones**:
- `metadata->>'doc_id'` ‚Üí `documents.doc_id` (relaci√≥n l√≥gica)

**Nota**: El sistema usa la funci√≥n RPC `match_documents_384` para b√∫squeda sem√°ntica en lugar de √≠ndices de LlamaIndex. Esta funci√≥n est√° optimizada para embeddings de 384 dimensiones.

### 6.3. Tabla `profiles` (Usuarios)

**Prop√≥sito**: Gesti√≥n de usuarios y tokens

**Columnas** (relevantes):
- `id` (UUID, PRIMARY KEY): ID del usuario (de Supabase Auth)
- `tokens_restantes` (INTEGER): Tokens disponibles para el usuario
- `email` (TEXT): Email del usuario

### 6.4. Tabla `conversations` (Historial)

**Prop√≥sito**: Historial de conversaciones

**Columnas**:
- `id` (UUID, PRIMARY KEY): ID √∫nico de la conversaci√≥n
- `user_id` (UUID, FOREIGN KEY): ID del usuario
- `message_role` (TEXT): 'user' o 'assistant'
- `message_content` (TEXT): Contenido del mensaje
- `tokens_used` (INTEGER): Tokens usados en esta respuesta
- `created_at` (TIMESTAMP, DEFAULT NOW()): Fecha de creaci√≥n

### 6.5. Esquema Completo

```
documents (doc_id, filename, file_path, title, total_chunks, ...)
    ‚Üì
book_chunks (id, embedding[384], content, metadata)
    ‚îî‚îÄ metadata->>'doc_id' referencia documents.doc_id
    ‚îî‚îÄ B√∫squeda mediante match_documents_384 RPC

profiles (id, tokens_restantes, email, ...)
    ‚Üì
conversations (id, user_id, message_role, message_content, ...)
    ‚îî‚îÄ user_id referencia profiles.id
```

---

## 7. Monitor de Ingesta y Reporte Final

### 7.1. Monitor en Tiempo Real

El sistema incluye un **monitor en tiempo real** que muestra:

**M√©tricas principales**:
- **Progreso**: Archivos procesados / total
- **Velocidad**: Archivos/minuto, chunks/minuto
- **ETA**: Tiempo estimado restante
- **RPM/TPM**: Requests y tokens por minuto (estimados)
- **Errores**: Contador de errores por tipo

**Actualizaciones**:
- Cada 5 segundos (configurable)
- Visualizaci√≥n con `rich` si est√° disponible
- Salida simple si `rich` no est√° disponible

**Thread-safe**:
- Usa locks para acceso concurrente
- Seguro para m√∫ltiples workers

### 7.2. M√©tricas Registradas

**Contadores globales**:
- `total_files`: Total de archivos a procesar
- `files_processed`: Archivos procesados exitosamente
- `files_failed`: Archivos con error total
- `files_suspicious`: Archivos con < 5 chunks
- `files_duplicated`: Archivos duplicados saltados
- `files_reindexed`: Archivos reindexados
- `total_chunks`: Chunks generados
- `rate_limit_retries`: Reintentos por error 429
- `network_errors`: Errores de red
- `other_errors`: Otros errores

**M√©tricas de velocidad**:
- `files_per_minute`: Archivos procesados por minuto
- `chunks_per_minute`: Chunks generados por minuto
- `estimated_rpm`: RPM estimado
- `estimated_tpm`: TPM estimado

**Calidad de datos**:
- `min_chunks_per_file`: M√≠nimo de chunks por archivo
- `max_chunks_per_file`: M√°ximo de chunks por archivo
- `avg_chunks_per_file`: Promedio de chunks por archivo
- `suspicious_files`: Lista de archivos sospechosos

### 7.3. Hooks del Monitor

El monitor proporciona hooks para registrar eventos:

```python
monitor.on_file_started(file_name, file_path)
monitor.on_file_completed(file_name, chunks_generated, is_suspicious=False)
monitor.on_file_error(file_name, error_message, error_type="other")
monitor.on_file_duplicated(file_name, doc_id)
monitor.on_file_reindexed(file_name, doc_id, deleted_chunks)
monitor.on_chunk_batch_processed(chunks_count, estimated_tokens)
```

### 7.4. Reporte Final

Al finalizar la ingesta, se genera un **reporte en formato Markdown** con:

**Informaci√≥n de ejecuci√≥n**:
- Fecha y hora de inicio
- Fecha y hora de finalizaci√≥n
- Tiempo total de ejecuci√≥n

**Resumen general**:
- Archivos totales
- Archivos procesados correctamente
- Archivos con errores
- Archivos sospechosos (< 5 chunks)
- Archivos duplicados saltados
- Archivos reindexados
- Chunks totales generados
- Promedio, m√≠nimo y m√°ximo de chunks por archivo

**Advertencias y problemas**:
- Lista de archivos duplicados saltados
- Lista de archivos reindexados
- Lista de archivos sospechosos
- Lista de archivos con error total

**M√©tricas de rendimiento**:
- Velocidad promedio (archivos/minuto, chunks/minuto)
- RPM estimado promedio
- TPM estimado promedio

**Distribuci√≥n de chunks**:
- Tabla con distribuci√≥n por rangos (0-5, 5-20, 20-50, etc.)

**Resumen de errores**:
- Reintentos por rate limit (429)
- Errores de red
- Otros errores

**Ubicaci√≥n del reporte**:
- Por defecto: `ingestion_report_YYYYMMDD_HHMMSS.md`
- Configurable mediante variable de entorno: `REPORT_FILE_PATH`

---

## 8. Metadatos y Filtros de B√∫squeda

### 8.1. Metadatos Guardados

Cada chunk guarda los siguientes metadatos en Supabase:

```json
{
  "file_name": "libro.pdf",
  "chunk_id": "abc123...",
  "doc_id": "def456...",
  "chunk_index": 0,
  "total_chunks": 100,
  "char_range": "0-1024",
  "book_title": "T√≠tulo del Libro"
}
```

**Campos**:
- `file_name`: Nombre del archivo
- `chunk_id`: ID √∫nico del chunk (hash determin√≠stico)
- `doc_id`: ID del documento (hash del archivo)
- `chunk_index`: √çndice del chunk en el documento (0-based)
- `total_chunks`: N√∫mero total de chunks del documento
- `char_range`: Rango de caracteres del chunk (start-end)
- `book_title`: T√≠tulo del libro/documento (opcional)

### 8.2. Filtros de B√∫squeda (Plan Futuro)

**Estado actual**: Los filtros por idioma, categor√≠a, autor, a√±o **no est√°n implementados** en el c√≥digo actual. Esto es una **mejora planificada para el futuro**.

**Metadatos que se podr√≠an agregar**:
- `language`: Idioma del documento (es, en, fr, etc.)
- `category`: Categor√≠a/tema (trading, finanzas, tecnolog√≠a, etc.)
- `author`: Autor del documento
- `year`: A√±o de publicaci√≥n
- `publisher`: Editorial

**Funcionalidad planificada**:
```python
def search_with_filters(
    query: str,
    language: Optional[str] = None,
    category: Optional[str] = None,
    author: Optional[str] = None,
    year_min: Optional[int] = None,
    year_max: Optional[int] = None
):
    # Filtrar chunks por metadatos antes de la b√∫squeda
    # Usar metadata->>'language', metadata->>'category', etc.
    pass
```

**Implementaci√≥n sugerida**:
1. Extraer metadatos del documento (t√≠tulo, autor, a√±o, etc.)
2. Guardar metadatos en la tabla `documents`
3. Agregar metadatos a cada chunk
4. Modificar el retriever para filtrar por metadatos
5. Actualizar la API para aceptar filtros

---

## 9. Gu√≠a R√°pida para Ejecutar el Sistema

### 9.1. Configurar Variables de Entorno

Crear archivo `.env` en la ra√≠z del proyecto:

```env
# Supabase
SUPABASE_URL=https://tu-proyecto.supabase.co
SUPABASE_SERVICE_KEY=tu-service-key
SUPABASE_DB_URL=postgresql://postgres.tu-proyecto:password@aws-0-us-west-1.pooler.supabase.com:5432/postgres
SUPABASE_DB_PASSWORD=tu-password

# OpenAI
OPENAI_API_KEY=tu-openai-api-key

# Opcional: Otros modelos de IA
DEEPSEEK_API_KEY=tu-deepseek-api-key
ANTHROPIC_API_KEY=tu-anthropic-api-key
GOOGLE_API_KEY=tu-google-api-key
COHERE_API_KEY=tu-cohere-api-key

# Opcional: URLs del frontend y backend
FRONTEND_URL=https://www.codextrader.tech
BACKEND_URL=https://api.codextrader.tech

# Opcional: Configuraci√≥n de ingesta
MAX_WORKERS=15
EMBEDDING_BATCH_SIZE=30
FORCE_REINDEX=false
LOG_LEVEL=INFO
```

### 9.2. Instalar Dependencias

```bash
pip install -r requirements.txt
```

**Dependencias principales**:
- `fastapi`: API REST
- `uvicorn`: Servidor ASGI
- `supabase`: Cliente de Supabase
- `litellm`: Abstracci√≥n para modelos de IA
- `llama-index`: Framework RAG
- `sentence-transformers`: Embeddings locales (all-MiniLM-L6-v2)
- `python-dotenv`: Variables de entorno
- `pypdf`: Lectura de PDFs
- `python-docx`: Lectura de documentos Word
- `ebooklib`: Lectura de EPUBs

### 9.3. Configurar Base de Datos

**Crear tablas en Supabase**:

1. **Tabla `documents`** (se crea autom√°ticamente al ejecutar `ingest_optimized_rag.py`)
2. **Colecci√≥n de vectores** (se crea autom√°ticamente al ejecutar la ingesta)
3. **Tabla `profiles`** (para usuarios y tokens)
4. **Tabla `conversations`** (para historial de conversaciones)

**Scripts SQL** (si necesitas crearlos manualmente):
- Ver `create_profiles_table.sql`
- Ver `create_conversations_table.sql`
- Ver `create_match_documents_384_function.sql` (funci√≥n RPC para b√∫squeda)

### 9.4. Preparar Archivos

Colocar archivos en la carpeta `./data/`:

```bash
mkdir data
# Copiar archivos PDF, EPUB, TXT, DOCX, MD a ./data/
```

### 9.5. Ejecutar Ingesta

**Opci√≥n 1: Ingesta mejorada (simple)**:
```bash
python ingest_improved.py
```

**Opci√≥n 2: Ingesta optimizada (con monitor y anti-duplicados)**:
```bash
python ingest_optimized_rag.py
```

**Opci√≥n 3: Forzar reindexaci√≥n**:
```bash
FORCE_REINDEX=true python ingest_optimized_rag.py
```

### 9.6. Ver Progreso

El monitor muestra el progreso en tiempo real:
- Progreso: Archivos procesados / total
- Velocidad: Archivos/minuto, chunks/minuto
- ETA: Tiempo estimado restante
- RPM/TPM: Requests y tokens por minuto

### 9.7. Revisar Reporte Final

Al finalizar, se genera un reporte en `ingestion_report_YYYYMMDD_HHMMSS.md` con:
- Resumen general
- Archivos procesados
- Archivos con errores
- Archivos sospechosos
- M√©tricas de rendimiento

### 9.8. Iniciar API de Consulta

```bash
python main.py
```

La API estar√° disponible en `http://localhost:8000`

**Endpoints**:
- `GET /`: Informaci√≥n de la API
- `GET /health`: Salud de la API
- `POST /chat`: Consulta RAG (requiere autenticaci√≥n)
- `GET /tokens`: Tokens restantes del usuario
- `POST /tokens/reload`: Recargar tokens
- `GET /conversations`: Historial de conversaciones

**Documentaci√≥n**: `http://localhost:8000/docs`

### 9.9. Comandos T√≠picos

```bash
# Verificar archivos nuevos
python check_new_files.py

# Verificar estado de la ingesta
python check_status.py

# Verificar datos en Supabase
python view_data.py

# Verificar duplicados
python check_duplicates.py

# Verificar l√≠mites de OpenAI
python verificar_limites_openai.py
```

---

## 10. Glosario de Conceptos B√°sicos

### RAG (Retrieval-Augmented Generation)

**Definici√≥n**: T√©cnica que combina b√∫squeda de informaci√≥n (retrieval) con generaci√≥n de texto (generation) para crear respuestas basadas en un corpus de documentos.

**C√≥mo funciona**:
1. El usuario hace una pregunta
2. El sistema busca los documentos m√°s relevantes
3. El sistema usa esos documentos como contexto
4. El LLM genera una respuesta basada en el contexto

**Ventajas**:
- Respuestas m√°s precisas y contextualizadas
- Puede citar fuentes espec√≠ficas
- Reduce alucinaciones del LLM

### Documento vs. Chunk

**Documento**: Archivo completo (ej: un PDF de 500 p√°ginas)

**Chunk**: Fragmento del documento (ej: 1024 caracteres del PDF)

**¬øPor qu√© dividir en chunks?**
- Los modelos de embeddings tienen l√≠mites de tama√±o
- Permite b√∫squeda m√°s precisa
- Mejora la recuperaci√≥n de informaci√≥n espec√≠fica

### Embedding

**Definici√≥n**: Representaci√≥n vectorial de un texto que captura su significado sem√°ntico.

**Caracter√≠sticas**:
- Es un vector de n√∫meros (ej: 384 dimensiones con all-MiniLM-L6-v2)
- Textos similares tienen embeddings similares
- Se usa para b√∫squeda sem√°ntica
- En este sistema: Generado localmente con SentenceTransformer

**Ejemplo**:
- "gato" y "felino" tienen embeddings similares
- "gato" y "perro" tienen embeddings m√°s similares que "gato" y "coche"

### Vector Store

**Definici√≥n**: Base de datos especializada en almacenar y buscar vectores (embeddings).

**Caracter√≠sticas**:
- Almacena embeddings y metadatos
- Permite b√∫squeda por similitud (distancia coseno)
- Optimizado para b√∫squeda sem√°ntica

**En este proyecto**: Supabase con pgvector (PostgreSQL)
- Tabla: `book_chunks`
- Funci√≥n de b√∫squeda: `match_documents_384`
- Dimensiones: 384 (all-MiniLM-L6-v2)

### doc_id

**Definici√≥n**: Identificador √∫nico de un documento basado en hash SHA256 del archivo.

**Caracter√≠sticas**:
- Determin√≠stico: siempre genera el mismo hash para el mismo archivo
- √önico: archivos diferentes tienen hashes diferentes
- Usado para detectar duplicados

### chunk_id

**Definici√≥n**: Identificador √∫nico de un chunk basado en hash del contenido.

**Caracter√≠sticas**:
- Determin√≠stico: siempre genera el mismo hash para el mismo contenido
- √önico: chunks diferentes tienen hashes diferentes
- Usado para evitar duplicar chunks

### match_documents_384

**Definici√≥n**: Funci√≥n RPC en Supabase que realiza b√∫squeda sem√°ntica usando embeddings de 384 dimensiones.

**Par√°metros**:
- `query_embedding`: Vector de 384 dimensiones de la consulta
- `match_count`: N√∫mero de chunks a retornar (por defecto: 5)
- `category_filter` (opcional): Filtrar por categor√≠a

**Retorna**: Lista de chunks ordenados por similitud (distancia coseno)

**Ventajas**:
- Optimizado para embeddings de 384 dimensiones
- B√∫squeda eficiente usando pgvector
- Soporte para filtros por categor√≠a

---

## 11. Ideas de Mejora Futura

### 11.1. Filtros de B√∫squeda Avanzados

**Estado**: Plan futuro

**Mejoras**:
- Filtros por idioma (es, en, fr, etc.)
- Filtros por categor√≠a/tema (trading, finanzas, tecnolog√≠a, etc.)
- Filtros por autor
- Filtros por rango de a√±os
- Filtros combinados (m√∫ltiples criterios)

### 11.2. Extracci√≥n de Metadatos Autom√°tica

**Estado**: Plan futuro

**Mejoras**:
- Extraer t√≠tulo, autor, a√±o, editorial autom√°ticamente del PDF
- Clasificaci√≥n autom√°tica por categor√≠a/tema
- Detecci√≥n autom√°tica de idioma
- Extracci√≥n de resumen/abstract

### 11.3. Rerankers

**Estado**: Plan futuro

**Mejoras**:
- Usar rerankers para mejorar la precisi√≥n de la b√∫squeda
- Reordenar resultados por relevancia
- Mejorar la calidad de las respuestas

### 11.4. Evaluaci√≥n de Calidad

**Estado**: Plan futuro

**Mejoras**:
- Evaluar la calidad de las respuestas con preguntas de test
- M√©tricas de precisi√≥n, recall, F1
- Comparar diferentes configuraciones

### 11.5. UI para Explorar la Biblioteca

**Estado**: Plan futuro

**Mejoras**:
- Interfaz web para explorar documentos indexados
- B√∫squeda visual
- Visualizaci√≥n de chunks
- Estad√≠sticas de la biblioteca

### 11.6. Mejoras de Rendimiento

**Estado**: Plan futuro

**Mejoras**:
- Cache de embeddings
- Procesamiento incremental (solo archivos nuevos)
- Optimizaci√≥n de queries en pgvector
- Compresi√≥n de embeddings

### 11.7. Mejoras de Seguridad

**Estado**: Plan futuro

**Mejoras**:
- Encriptaci√≥n de documentos sensibles
- Control de acceso por usuario
- Auditor√≠a de consultas
- Logs de seguridad

---

## 12. Archivos Principales del Proyecto

### 12.1. Ingesta

- **`ingest_improved.py`**: Ingesta simple y directa
- **`ingest_optimized_rag.py`**: Ingesta optimizada con monitor y anti-duplicados
- **`config_ingesta.py`**: Configuraci√≥n centralizada de ingesta
- **`anti_duplicates.py`**: Sistema anti-duplicados
- **`ingestion_monitor.py`**: Monitor de ingesta en tiempo real

### 12.2. API y Consulta

- **`main.py`**: API FastAPI para consultas RAG
- **`config.py`**: Configuraci√≥n general del proyecto

### 12.3. Utilidades

- **`check_new_files.py`**: Verificar archivos nuevos
- **`check_status.py`**: Verificar estado de la ingesta
- **`view_data.py`**: Ver datos en Supabase
- **`check_duplicates.py`**: Verificar duplicados
- **`verificar_limites_openai.py`**: Verificar l√≠mites de OpenAI

### 12.4. Documentaci√≥n

- **`README.md`**: Este archivo
- **`PIPELINE_TECNICO.md`**: Documentaci√≥n t√©cnica del pipeline
- **`RESUMEN_ANTI_DUPLICADOS.md`**: Resumen del sistema anti-duplicados
- **`RESUMEN_MONITOR_REPORTE.md`**: Resumen del monitor y reporte

---

## 13. Conclusi√≥n

Este sistema RAG de ingesta de libros es una soluci√≥n completa y robusta para indexar y consultar grandes colecciones de documentos. Incluye:

- ‚úÖ **Ingesta masiva y paralela**
- ‚úÖ **Embeddings locales sin costo** (SentenceTransformer)
- ‚úÖ **Sistema anti-duplicados robusto**
- ‚úÖ **Monitor en tiempo real**
- ‚úÖ **Reporte detallado**
- ‚úÖ **API REST para consultas**
- ‚úÖ **Soporte para m√∫ltiples modelos de IA**
- ‚úÖ **B√∫squeda sem√°ntica optimizada** (match_documents_384)

**Pr√≥ximos pasos**:
1. Implementar filtros de b√∫squeda avanzados
2. Mejorar extracci√≥n de metadatos
3. Agregar rerankers
4. Crear UI para explorar la biblioteca

---

## 14. Contacto y Soporte

Para preguntas o problemas, consulta la documentaci√≥n t√©cnica en los archivos `.md` del proyecto o revisa el c√≥digo fuente directamente.

**Archivos de referencia**:
- `PIPELINE_TECNICO.md`: Pipeline t√©cnico completo
- `RESUMEN_ANTI_DUPLICADOS.md`: Sistema anti-duplicados
- `RESUMEN_MONITOR_REPORTE.md`: Monitor y reporte
- `GUIA_MONITOR_REPORTE.md`: Gu√≠a de uso del monitor

---

## 15. Cambios recientes

### 2025-11-19: Migraci√≥n a Embeddings Locales

- **Migraci√≥n completa a SentenceTransformer**: El sistema ahora usa `all-MiniLM-L6-v2` (384 dimensiones) en lugar de OpenAI embeddings
- **Sin costo por embeddings**: El procesamiento es completamente local, sin l√≠mites de API
- **Funci√≥n RPC optimizada**: Uso de `match_documents_384` para b√∫squeda sem√°ntica en Supabase
- **Tabla actualizada**: Los embeddings se almacenan en `book_chunks` con 384 dimensiones

### 2025-11-16: Mejoras de Rendimiento

- Escalado de Supabase al plan XL (4 cores, 16 GB RAM) para mayor concurrencia.
- Instalaci√≥n de `tenacity` en `venv_ingesta` para reintentos autom√°ticos.
- Reemplazo completo de `ingest_masiva_local.py` por versi√≥n "anti-fracaso":
  - Reintentos robustos con `RemoteProtocolError`, `ConnectError`, `httpx.ReadError` y `httpcore.ReadError`.
  - `upsert(..., on_conflict="doc_id")` en tabla `documents` para evitar `23505 duplicate key`.
  - Limpieza de texto eliminando `\u0000` para evitar error `22P05` en Postgres.
  - Par√°metros de alto rendimiento para XL: `MAX_WORKERS_LECTURA=20`, `DB_INSERT_BATCH_SIZE=250`, `EMBEDDING_BATCH_SIZE=256`, `CHUNK_SIZE=1000`, `CHUNK_OVERLAP=200`.
  - Correcci√≥n de import: `langchain_text_splitters.RecursiveCharacterTextSplitter`.
- Ejecuci√≥n reciente completada en ~23.42 minutos, con reintentos aplicados y progreso continuo ante fallos de red puntuales.

---

**√öltima actualizaci√≥n**: 2025-11-19
