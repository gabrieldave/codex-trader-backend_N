# üìö Sistema RAG de Ingesta de Libros

## 1. Introducci√≥n General

### ¬øQu√© es este proyecto?

Este proyecto implementa un sistema completo de **RAG (Retrieval-Augmented Generation)** para procesar, indexar y consultar documentos (principalmente libros en formato PDF, EPUB, TXT, DOCX, MD) usando embeddings vectoriales y b√∫squeda sem√°ntica.

### ¬øQu√© problema resuelve?

El sistema resuelve el problema de **indexar grandes colecciones de libros/documentos** y permitir consultas sem√°nticas sobre su contenido:

- **Ingesta masiva**: Procesa cientos o miles de archivos de forma eficiente y paralela
- **B√∫squeda sem√°ntica**: Permite hacer preguntas en lenguaje natural sobre el contenido indexado
- **Anti-duplicados**: Evita indexar el mismo contenido dos veces
- **Control de l√≠mites**: Respeta los l√≠mites de la API de OpenAI (Tier 3) sin excederlos
- **Monitoreo en tiempo real**: Muestra progreso, velocidad y m√©tricas durante la ingesta
- **Reporte detallado**: Genera un reporte final con estad√≠sticas completas

### Tecnolog√≠as Principales

- **Python 3.x** - Lenguaje principal
- **LlamaIndex** - Framework para RAG y procesamiento de documentos
- **OpenAI** - Embeddings con modelo `text-embedding-3-small` (1536 dimensiones)
- **Supabase** - Base de datos PostgreSQL con extensi√≥n pgvector para almacenamiento vectorial
- **FastAPI** - API REST para consultas RAG
- **LiteLLM** - Abstracci√≥n para usar m√∫ltiples modelos de IA (OpenAI, Deepseek, Claude, Gemini, etc.)

---

## 2. Arquitectura General del Sistema

### Flujo End-to-End

El sistema funciona en dos fases principales:

#### **Fase 1: Ingesta de Documentos**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE DE INGESTI√ìN                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1Ô∏è‚É£ LEER ARCHIVOS
   ‚Üì
   SimpleDirectoryReader(input_files=[file_path])
   ‚Ä¢ Soporta: PDF, EPUB, TXT, DOCX, MD
   ‚Ä¢ Lee desde ./data/
   ‚Ä¢ Convierte autom√°ticamente a texto

2Ô∏è‚É£ VERIFICAR DUPLICADOS
   ‚Üì
   calculate_doc_id(file_path)  # Hash SHA256
   ‚Ä¢ Consulta tabla documents en Supabase
   ‚Ä¢ Si existe ‚Üí SKIP o REINDEX (seg√∫n configuraci√≥n)
   ‚Ä¢ Si no existe ‚Üí PROCESS

3Ô∏è‚É£ EXTRAER TEXTO
   ‚Üì
   reader.load_data()
   ‚Ä¢ LlamaIndex extrae texto autom√°ticamente
   ‚Ä¢ Crea objetos Document con metadata

4Ô∏è‚É£ DIVIDIR EN CHUNKS
   ‚Üì
   SentenceSplitter(chunk_size=1024, chunk_overlap=200)
   ‚Ä¢ Divide en chunks de 1024 caracteres
   ‚Ä¢ Overlap de 200 caracteres entre chunks
   ‚Ä¢ Mantiene contexto entre chunks adyacentes

5Ô∏è‚É£ GENERAR EMBEDDINGS
   ‚Üì
   OpenAIEmbedding(model="text-embedding-3-small")
   ‚Ä¢ Env√≠a chunks en batches de 30-40
   ‚Ä¢ Genera embeddings de 1536 dimensiones
   ‚Ä¢ Controla rate limits (70% de Tier 3)

6Ô∏è‚É£ GUARDAR EN SUPABASE
   ‚Üì
   SupabaseVectorStore + pgvector
   ‚Ä¢ Almacena vectores en PostgreSQL
   ‚Ä¢ Guarda metadata (file_name, chunk_id, doc_id, etc.)
   ‚Ä¢ Tabla: vecs.knowledge (configurable)
```

#### **Fase 2: Consulta RAG**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE DE CONSULTA                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1Ô∏è‚É£ RECIBIR PREGUNTA
   ‚Üì
   POST /chat { "query": "¬øQu√© dice el libro sobre X?" }

2Ô∏è‚É£ GENERAR EMBEDDING DE LA PREGUNTA
   ‚Üì
   OpenAIEmbedding(query)
   ‚Ä¢ Mismo modelo: text-embedding-3-small
   ‚Ä¢ 1536 dimensiones

3Ô∏è‚É£ B√öSQUEDA SEM√ÅNTICA
   ‚Üì
   VectorStoreIndex.as_retriever(similarity_top_k=5)
   ‚Ä¢ Busca los 5 chunks m√°s similares
   ‚Ä¢ Usa distancia coseno en pgvector

4Ô∏è‚É£ CONSTRUIR CONTEXTO
   ‚Üì
   Concatenar chunks recuperados
   ‚Ä¢ Crea contexto para el LLM

5Ô∏è‚É£ GENERAR RESPUESTA
   ‚Üì
   LiteLLM (OpenAI/Deepseek/Claude/Gemini)
   ‚Ä¢ Env√≠a contexto + pregunta al LLM
   ‚Ä¢ Genera respuesta basada en el contexto

6Ô∏è‚É£ DEVOLVER RESPUESTA
   ‚Üì
   { "response": "...", "tokens_usados": 150, "tokens_restantes": 19850 }
```

### Diagrama de Alto Nivel

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Libros/PDFs ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ   Ingesta    ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ   Chunks     ‚îÇ
‚îÇ   ./data/    ‚îÇ     ‚îÇ  (Paralelo)  ‚îÇ     ‚îÇ  (1024 chars)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                   ‚îÇ
                                                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Consulta   ‚îÇ ‚Üê‚îÄ‚îÄ ‚îÇ   Supabase   ‚îÇ ‚Üê‚îÄ‚îÄ ‚îÇ  Embeddings  ‚îÇ
‚îÇ   RAG API    ‚îÇ     ‚îÇ  (pgvector)  ‚îÇ     ‚îÇ  (1536 dims) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 3. Detalle de la Ingesta de Documentos

### 3.1. Lectura de Archivos

El sistema usa **LlamaIndex SimpleDirectoryReader** para leer archivos:

```python
reader = SimpleDirectoryReader(input_files=[file_path])
documents = reader.load_data()
```

**Formatos soportados**:
- PDF (`.pdf`)
- EPUB (`.epub`)
- Texto plano (`.txt`)
- Word (`.docx`)
- Markdown (`.md`)

**Ubicaci√≥n**: Los archivos se leen desde `./data/` (configurable en `config.py`)

### 3.2. Extracci√≥n de Texto

LlamaIndex extrae el texto autom√°ticamente seg√∫n el formato:
- **PDF**: Extrae texto de todas las p√°ginas
- **EPUB**: Extrae texto de todos los cap√≠tulos
- **TXT/MD**: Lee el contenido directamente
- **DOCX**: Extrae texto del documento Word

Cada archivo se convierte en uno o m√°s objetos `Document` de LlamaIndex.

### 3.3. Configuraci√≥n de Chunks

El sistema usa **chunking fijo** (no configurable sin solicitud expl√≠cita):

- **Chunk size**: **1024 caracteres** (no tokens)
- **Chunk overlap**: **200 caracteres** (~20% de overlap)
- **Splitter**: `SentenceSplitter` de LlamaIndex

**¬øPor qu√© 1024 caracteres?**
- Equivale a aproximadamente **256 tokens** (1 token ‚âà 4 caracteres)
- Balance entre contexto y precisi√≥n
- Compatible con el modelo de embeddings

**¬øPor qu√© 200 caracteres de overlap?**
- Mantiene contexto entre chunks adyacentes
- Evita cortar frases a la mitad
- Mejora la recuperaci√≥n de informaci√≥n

**C√≥digo**:
```python
text_splitter = SentenceSplitter(
    chunk_size=1024,      # caracteres
    chunk_overlap=200     # caracteres
)
```

### 3.4. Modelo de Embeddings

**Modelo**: `text-embedding-3-small` de OpenAI

**Caracter√≠sticas**:
- **Dimensiones**: 1536
- **Costo**: Muy econ√≥mico (mucho m√°s barato que `text-embedding-3-large`)
- **Calidad**: Excelente para la mayor√≠a de casos de uso
- **Velocidad**: Muy r√°pido

**¬øPor qu√© este modelo?**
- Balance perfecto entre costo y calidad
- Suficiente para b√∫squeda sem√°ntica en libros
- Compatible con pgvector en Supabase

**C√≥digo**:
```python
embed_model = OpenAIEmbedding(model="text-embedding-3-small")
```

### 3.5. Batch Size de Embeddings

**Configuraci√≥n**: 30-40 chunks por request (por defecto: 30)

**¬øPor qu√© este batch size?**
- Optimizado para Tier 3 de OpenAI
- Respeta l√≠mites de RPM y TPM
- Balance entre velocidad y control de rate limits

**Control de rate limits**:
- El sistema usa un `RateLimiter` que controla:
  - **RPM**: Requests por minuto
  - **TPM**: Tokens por minuto
- Objetivo: Usar solo **70% de la capacidad** de Tier 3

**C√≥digo**:
```python
EMBEDDING_BATCH_SIZE = 30  # chunks por request
```

### 3.6. Workers y Procesamiento Paralelo

**N√∫mero de workers por defecto**: **15**

**Configuraci√≥n**:
- Configurable mediante variable de entorno: `MAX_WORKERS=15`
- Usa `ThreadPoolExecutor` para concurrencia
- Cada worker procesa un archivo a la vez

**¬øPor qu√© 15 workers?**
- Balance entre velocidad y control de recursos
- Evita saturar la API de OpenAI
- Mantiene el uso por debajo del 70% de Tier 3

**C√≥digo**:
```python
MAX_WORKERS = 15  # configurable
executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)
```

---

## 4. Control de L√≠mites (Tier 3) y Rendimiento

### 4.1. L√≠mites de OpenAI Tier 3

El sistema est√° configurado para usar **OpenAI Tier 3** con los siguientes l√≠mites:

- **RPM (Requests Per Minute)**: **5,000**
- **TPM (Tokens Per Minute)**: **5,000,000**
- **TPD (Tokens Per Day)**: **100,000,000**

### 4.2. Objetivo de Rendimiento: 70% de Capacidad

**¬øPor qu√© usar solo 70%?**
- **Margen de seguridad**: Evita exceder l√≠mites por picos inesperados
- **Estabilidad**: Reduce errores 429 (rate limit exceeded)
- **Confiabilidad**: Permite procesar archivos grandes sin problemas

**Objetivos**:
- **RPM objetivo**: **3,500** (70% de 5,000)
- **TPM objetivo**: **3,500,000** (70% de 5,000,000)

### 4.3. Control de Rate Limits

El sistema implementa un **RateLimiter** que:

1. **Monitorea RPM y TPM en tiempo real**
2. **Espera si es necesario** antes de hacer requests
3. **Maneja errores 429** con backoff exponencial
4. **Reintenta autom√°ticamente** (m√°ximo 5 intentos)

**Backoff exponencial**:
- Intento 1: Espera 1 segundo
- Intento 2: Espera 2 segundos
- Intento 3: Espera 4 segundos
- Intento 4: Espera 8 segundos
- Intento 5: Espera 16 segundos

**C√≥digo**:
```python
class RateLimiter:
    def wait_if_needed(self):
        # Verifica RPM y TPM
        # Espera si es necesario
        # Responde errores 429 con backoff
```

### 4.4. Distribuci√≥n del Trabajo

El sistema distribuye el trabajo de la siguiente manera:

1. **M√∫ltiples workers** procesan archivos en paralelo
2. **Cada worker** procesa un archivo completo
3. **Cada archivo** se divide en chunks
4. **Cada batch** de chunks se env√≠a a OpenAI
5. **RateLimiter** controla la velocidad global

**Ejemplo**:
- 15 workers procesando en paralelo
- Cada worker procesa ~10 archivos/minuto
- Total: ~150 archivos/minuto
- Chunks: ~15,000 chunks/minuto
- RPM: ~500 requests/minuto (muy por debajo de 3,500)

---

## 5. L√≥gica Anti-Duplicados

### 5.1. Detecci√≥n de Duplicados a Nivel de Documento

El sistema usa **hash SHA256 del archivo** para detectar duplicados:

**C√°lculo de `doc_id`**:
```python
doc_id = calculate_doc_id(file_path)  # SHA256 de los bytes del archivo
```

**Ventajas**:
- Detecta duplicados incluso si el archivo tiene diferente nombre
- Detecta contenido id√©ntico en archivos diferentes
- Determin√≠stico: siempre genera el mismo hash para el mismo archivo

**Alternativa disponible**:
```python
doc_id = calculate_doc_id(file_path, use_content_hash=True, content=texto)
```
- Usa hash del contenido normalizado (√∫til para detectar contenido duplicado con formato diferente)

### 5.2. Tabla `documents` en Supabase

**Estructura**:
```sql
CREATE TABLE documents (
    doc_id TEXT PRIMARY KEY,
    filename TEXT NOT NULL,
    file_path TEXT,
    title TEXT,
    hash_method TEXT DEFAULT 'sha256',
    total_chunks INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
)
```

**√çndices**:
- `idx_documents_filename` en `filename`
- `idx_documents_created_at` en `created_at`

**Prop√≥sito**:
- Tracking de documentos indexados
- Verificaci√≥n r√°pida de duplicados
- Auditor√≠a de ingesta

### 5.3. Decisi√≥n de Procesamiento

Antes de procesar un archivo, el sistema toma una decisi√≥n:

```python
action, existing_doc = decide_document_action(doc_id, force_reindex=FORCE_REINDEX)

if action == "skip":
    # Duplicado detectado, saltar
    monitor.on_file_duplicated(file_name, doc_id)
elif action == "reindex":
    # Eliminar chunks anteriores y reindexar
    delete_document_chunks(doc_id, collection_name)
    # Procesar archivo normalmente
elif action == "process":
    # Nuevo documento, procesar normalmente
```

**Configuraci√≥n**:
- **Variable de entorno**: `FORCE_REINDEX=false` (por defecto)
- Si `FORCE_REINDEX=true`: Fuerza reindexaci√≥n incluso si el documento existe

### 5.4. Detecci√≥n de Duplicados a Nivel de Chunk

**C√°lculo de `chunk_id`**:
```python
chunk_id = calculate_chunk_id(doc_id, chunk_index, chunk_content)
# Hash de: doc_id + ":" + chunk_index + ":" + contenido_normalizado
```

**Verificaci√≥n**:
Antes de procesar cada batch, se verifica si el chunk ya existe:

```python
if check_chunk_exists(chunk_id, collection_name):
    # Chunk duplicado, saltar
    continue
```

**Ventajas**:
- Evita duplicar chunks individuales
- √ötil cuando se reindexa un documento parcialmente
- Determ√≠nistico: siempre genera el mismo `chunk_id` para el mismo contenido

### 5.5. Flujo Completo Anti-Duplicados

```
1. Calcular doc_id (hash del archivo)
   ‚Üì
2. Verificar en tabla documents
   ‚Üì
3a. Si existe y FORCE_REINDEX=False ‚Üí SKIP (duplicado)
3b. Si existe y FORCE_REINDEX=True ‚Üí REINDEX (eliminar chunks y procesar)
3c. Si no existe ‚Üí PROCESS (nuevo)
   ‚Üì
4. Procesar archivo (si no es skip)
   ‚Üì
5. Para cada chunk:
   - Calcular chunk_id determin√≠stico
   - Verificar si chunk existe
   - Si existe ‚Üí saltar chunk
   - Si no existe ‚Üí procesar
   ‚Üì
6. Registrar documento en tabla documents
```

---

## 6. Estructura de la Base de Datos en Supabase

### 6.1. Tabla `documents`

**Prop√≥sito**: Tracking de documentos indexados

**Columnas**:
- `doc_id` (TEXT, PRIMARY KEY): Hash SHA256 del archivo
- `filename` (TEXT, NOT NULL): Nombre del archivo
- `file_path` (TEXT): Ruta completa del archivo
- `title` (TEXT): T√≠tulo del documento (opcional)
- `hash_method` (TEXT, DEFAULT 'sha256'): M√©todo de hash usado
- `total_chunks` (INTEGER, DEFAULT 0): N√∫mero total de chunks
- `created_at` (TIMESTAMP, DEFAULT NOW()): Fecha de creaci√≥n
- `updated_at` (TIMESTAMP, DEFAULT NOW()): Fecha de √∫ltima actualizaci√≥n

**√çndices**:
- `idx_documents_filename`: B√∫squeda r√°pida por nombre de archivo
- `idx_documents_created_at`: Ordenamiento por fecha

### 6.2. Tabla `vecs.knowledge` (Colecci√≥n de Vectores)

**Prop√≥sito**: Almacenamiento de embeddings y chunks

**Estructura** (pgvector):
- `id` (UUID, PRIMARY KEY): ID √∫nico del chunk
- `embedding` (vector(1536)): Embedding vectorial (1536 dimensiones)
- `metadata` (JSONB): Metadatos del chunk

**Metadatos guardados**:
```json
{
  "file_name": "libro.pdf",
  "chunk_id": "abc123...",
  "doc_id": "def456...",
  "chunk_index": 0,
  "total_chunks": 100,
  "char_range": "0-1024",
  "book_title": "T√≠tulo del Libro"
}
```

**Relaciones**:
- `metadata->>'doc_id'` ‚Üí `documents.doc_id` (relaci√≥n l√≥gica)

### 6.3. Tabla `profiles` (Usuarios)

**Prop√≥sito**: Gesti√≥n de usuarios y tokens

**Columnas** (relevantes):
- `id` (UUID, PRIMARY KEY): ID del usuario (de Supabase Auth)
- `tokens_restantes` (INTEGER): Tokens disponibles para el usuario
- `email` (TEXT): Email del usuario

### 6.4. Tabla `conversations` (Historial)

**Prop√≥sito**: Historial de conversaciones

**Columnas**:
- `id` (UUID, PRIMARY KEY): ID √∫nico de la conversaci√≥n
- `user_id` (UUID, FOREIGN KEY): ID del usuario
- `message_role` (TEXT): 'user' o 'assistant'
- `message_content` (TEXT): Contenido del mensaje
- `tokens_used` (INTEGER): Tokens usados en esta respuesta
- `created_at` (TIMESTAMP, DEFAULT NOW()): Fecha de creaci√≥n

### 6.5. Esquema Completo

```
documents (doc_id, filename, file_path, title, total_chunks, ...)
    ‚Üì
vecs.knowledge (id, embedding, metadata)
    ‚îî‚îÄ metadata->>'doc_id' referencia documents.doc_id

profiles (id, tokens_restantes, email, ...)
    ‚Üì
conversations (id, user_id, message_role, message_content, ...)
    ‚îî‚îÄ user_id referencia profiles.id
```

---

## 7. Monitor de Ingesta y Reporte Final

### 7.1. Monitor en Tiempo Real

El sistema incluye un **monitor en tiempo real** que muestra:

**M√©tricas principales**:
- **Progreso**: Archivos procesados / total
- **Velocidad**: Archivos/minuto, chunks/minuto
- **ETA**: Tiempo estimado restante
- **RPM/TPM**: Requests y tokens por minuto (estimados)
- **Errores**: Contador de errores por tipo

**Actualizaciones**:
- Cada 5 segundos (configurable)
- Visualizaci√≥n con `rich` si est√° disponible
- Salida simple si `rich` no est√° disponible

**Thread-safe**:
- Usa locks para acceso concurrente
- Seguro para m√∫ltiples workers

### 7.2. M√©tricas Registradas

**Contadores globales**:
- `total_files`: Total de archivos a procesar
- `files_processed`: Archivos procesados exitosamente
- `files_failed`: Archivos con error total
- `files_suspicious`: Archivos con < 5 chunks
- `files_duplicated`: Archivos duplicados saltados
- `files_reindexed`: Archivos reindexados
- `total_chunks`: Chunks generados
- `rate_limit_retries`: Reintentos por error 429
- `network_errors`: Errores de red
- `other_errors`: Otros errores

**M√©tricas de velocidad**:
- `files_per_minute`: Archivos procesados por minuto
- `chunks_per_minute`: Chunks generados por minuto
- `estimated_rpm`: RPM estimado
- `estimated_tpm`: TPM estimado

**Calidad de datos**:
- `min_chunks_per_file`: M√≠nimo de chunks por archivo
- `max_chunks_per_file`: M√°ximo de chunks por archivo
- `avg_chunks_per_file`: Promedio de chunks por archivo
- `suspicious_files`: Lista de archivos sospechosos

### 7.3. Hooks del Monitor

El monitor proporciona hooks para registrar eventos:

```python
monitor.on_file_started(file_name, file_path)
monitor.on_file_completed(file_name, chunks_generated, is_suspicious=False)
monitor.on_file_error(file_name, error_message, error_type="other")
monitor.on_file_duplicated(file_name, doc_id)
monitor.on_file_reindexed(file_name, doc_id, deleted_chunks)
monitor.on_chunk_batch_processed(chunks_count, estimated_tokens)
```

### 7.4. Reporte Final

Al finalizar la ingesta, se genera un **reporte en formato Markdown** con:

**Informaci√≥n de ejecuci√≥n**:
- Fecha y hora de inicio
- Fecha y hora de finalizaci√≥n
- Tiempo total de ejecuci√≥n

**Resumen general**:
- Archivos totales
- Archivos procesados correctamente
- Archivos con errores
- Archivos sospechosos (< 5 chunks)
- Archivos duplicados saltados
- Archivos reindexados
- Chunks totales generados
- Promedio, m√≠nimo y m√°ximo de chunks por archivo

**Advertencias y problemas**:
- Lista de archivos duplicados saltados
- Lista de archivos reindexados
- Lista de archivos sospechosos
- Lista de archivos con error total

**M√©tricas de rendimiento**:
- Velocidad promedio (archivos/minuto, chunks/minuto)
- RPM estimado promedio
- TPM estimado promedio

**Distribuci√≥n de chunks**:
- Tabla con distribuci√≥n por rangos (0-5, 5-20, 20-50, etc.)

**Resumen de errores**:
- Reintentos por rate limit (429)
- Errores de red
- Otros errores

**Ubicaci√≥n del reporte**:
- Por defecto: `ingestion_report_YYYYMMDD_HHMMSS.md`
- Configurable mediante variable de entorno: `REPORT_FILE_PATH`

---

## 8. Metadatos y Filtros de B√∫squeda

### 8.1. Metadatos Guardados

Cada chunk guarda los siguientes metadatos en Supabase:

```json
{
  "file_name": "libro.pdf",
  "chunk_id": "abc123...",
  "doc_id": "def456...",
  "chunk_index": 0,
  "total_chunks": 100,
  "char_range": "0-1024",
  "book_title": "T√≠tulo del Libro"
}
```

**Campos**:
- `file_name`: Nombre del archivo
- `chunk_id`: ID √∫nico del chunk (hash determin√≠stico)
- `doc_id`: ID del documento (hash del archivo)
- `chunk_index`: √çndice del chunk en el documento (0-based)
- `total_chunks`: N√∫mero total de chunks del documento
- `char_range`: Rango de caracteres del chunk (start-end)
- `book_title`: T√≠tulo del libro/documento (opcional)

### 8.2. Filtros de B√∫squeda (Plan Futuro)

**Estado actual**: Los filtros por idioma, categor√≠a, autor, a√±o **no est√°n implementados** en el c√≥digo actual. Esto es una **mejora planificada para el futuro**.

**Metadatos que se podr√≠an agregar**:
- `language`: Idioma del documento (es, en, fr, etc.)
- `category`: Categor√≠a/tema (trading, finanzas, tecnolog√≠a, etc.)
- `author`: Autor del documento
- `year`: A√±o de publicaci√≥n
- `publisher`: Editorial

**Funcionalidad planificada**:
```python
def search_with_filters(
    query: str,
    language: Optional[str] = None,
    category: Optional[str] = None,
    author: Optional[str] = None,
    year_min: Optional[int] = None,
    year_max: Optional[int] = None
):
    # Filtrar chunks por metadatos antes de la b√∫squeda
    # Usar metadata->>'language', metadata->>'category', etc.
    pass
```

**Implementaci√≥n sugerida**:
1. Extraer metadatos del documento (t√≠tulo, autor, a√±o, etc.)
2. Guardar metadatos en la tabla `documents`
3. Agregar metadatos a cada chunk
4. Modificar el retriever para filtrar por metadatos
5. Actualizar la API para aceptar filtros

---

## 9. Gu√≠a R√°pida para Ejecutar el Sistema

### 9.1. Configurar Variables de Entorno

Crear archivo `.env` en la ra√≠z del proyecto:

```env
# Supabase
SUPABASE_URL=https://tu-proyecto.supabase.co
SUPABASE_SERVICE_KEY=tu-service-key
SUPABASE_DB_PASSWORD=tu-password

# OpenAI
OPENAI_API_KEY=tu-openai-api-key

# Opcional: Otros modelos de IA
DEEPSEEK_API_KEY=tu-deepseek-api-key
ANTHROPIC_API_KEY=tu-anthropic-api-key
GOOGLE_API_KEY=tu-google-api-key
COHERE_API_KEY=tu-cohere-api-key

# Opcional: Configuraci√≥n de ingesta
MAX_WORKERS=15
EMBEDDING_BATCH_SIZE=30
FORCE_REINDEX=false
LOG_LEVEL=INFO
```

### 9.2. Instalar Dependencias

```bash
pip install -r requirements.txt
```

**Dependencias principales**:
- `fastapi`: API REST
- `uvicorn`: Servidor ASGI
- `supabase`: Cliente de Supabase
- `litellm`: Abstracci√≥n para modelos de IA
- `llama-index`: Framework RAG
- `llama-index-vector-stores-supabase`: Integraci√≥n con Supabase
- `llama-index-embeddings-openai`: Embeddings de OpenAI
- `python-dotenv`: Variables de entorno
- `pypdf`: Lectura de PDFs
- `python-docx`: Lectura de documentos Word
- `ebooklib`: Lectura de EPUBs

### 9.3. Configurar Base de Datos

**Crear tablas en Supabase**:

1. **Tabla `documents`** (se crea autom√°ticamente al ejecutar `ingest_optimized_rag.py`)
2. **Colecci√≥n de vectores** (se crea autom√°ticamente al ejecutar la ingesta)
3. **Tabla `profiles`** (para usuarios y tokens)
4. **Tabla `conversations`** (para historial de conversaciones)

**Scripts SQL** (si necesitas crearlos manualmente):
- Ver `create_profiles_table.sql`
- Ver `create_conversations_table.sql`

### 9.4. Preparar Archivos

Colocar archivos en la carpeta `./data/`:

```bash
mkdir data
# Copiar archivos PDF, EPUB, TXT, DOCX, MD a ./data/
```

### 9.5. Ejecutar Ingesta

**Opci√≥n 1: Ingesta mejorada (simple)**:
```bash
python ingest_improved.py
```

**Opci√≥n 2: Ingesta optimizada (con monitor y anti-duplicados)**:
```bash
python ingest_optimized_rag.py
```

**Opci√≥n 3: Forzar reindexaci√≥n**:
```bash
FORCE_REINDEX=true python ingest_optimized_rag.py
```

### 9.6. Ver Progreso

El monitor muestra el progreso en tiempo real:
- Progreso: Archivos procesados / total
- Velocidad: Archivos/minuto, chunks/minuto
- ETA: Tiempo estimado restante
- RPM/TPM: Requests y tokens por minuto

### 9.7. Revisar Reporte Final

Al finalizar, se genera un reporte en `ingestion_report_YYYYMMDD_HHMMSS.md` con:
- Resumen general
- Archivos procesados
- Archivos con errores
- Archivos sospechosos
- M√©tricas de rendimiento

### 9.8. Iniciar API de Consulta

```bash
python main.py
```

La API estar√° disponible en `http://localhost:8000`

**Endpoints**:
- `GET /`: Informaci√≥n de la API
- `GET /health`: Salud de la API
- `POST /chat`: Consulta RAG (requiere autenticaci√≥n)
- `GET /tokens`: Tokens restantes del usuario
- `POST /tokens/reload`: Recargar tokens
- `GET /conversations`: Historial de conversaciones

**Documentaci√≥n**: `http://localhost:8000/docs`

### 9.9. Comandos T√≠picos

```bash
# Verificar archivos nuevos
python check_new_files.py

# Verificar estado de la ingesta
python check_status.py

# Verificar datos en Supabase
python view_data.py

# Verificar duplicados
python check_duplicates.py

# Verificar l√≠mites de OpenAI
python verificar_limites_openai.py
```

---

## 10. Glosario de Conceptos B√°sicos

### RAG (Retrieval-Augmented Generation)

**Definici√≥n**: T√©cnica que combina b√∫squeda de informaci√≥n (retrieval) con generaci√≥n de texto (generation) para crear respuestas basadas en un corpus de documentos.

**C√≥mo funciona**:
1. El usuario hace una pregunta
2. El sistema busca los documentos m√°s relevantes
3. El sistema usa esos documentos como contexto
4. El LLM genera una respuesta basada en el contexto

**Ventajas**:
- Respuestas m√°s precisas y contextualizadas
- Puede citar fuentes espec√≠ficas
- Reduce alucinaciones del LLM

### Documento vs. Chunk

**Documento**: Archivo completo (ej: un PDF de 500 p√°ginas)

**Chunk**: Fragmento del documento (ej: 1024 caracteres del PDF)

**¬øPor qu√© dividir en chunks?**
- Los modelos de embeddings tienen l√≠mites de tama√±o
- Permite b√∫squeda m√°s precisa
- Mejora la recuperaci√≥n de informaci√≥n espec√≠fica

### Embedding

**Definici√≥n**: Representaci√≥n vectorial de un texto que captura su significado sem√°ntico.

**Caracter√≠sticas**:
- Es un vector de n√∫meros (ej: 1536 dimensiones)
- Textos similares tienen embeddings similares
- Se usa para b√∫squeda sem√°ntica

**Ejemplo**:
- "gato" y "felino" tienen embeddings similares
- "gato" y "perro" tienen embeddings m√°s similares que "gato" y "coche"

### Vector Store

**Definici√≥n**: Base de datos especializada en almacenar y buscar vectores (embeddings).

**Caracter√≠sticas**:
- Almacena embeddings y metadatos
- Permite b√∫squeda por similitud (distancia coseno)
- Optimizado para b√∫squeda sem√°ntica

**En este proyecto**: Supabase con pgvector (PostgreSQL)

### doc_id

**Definici√≥n**: Identificador √∫nico de un documento basado en hash SHA256 del archivo.

**Caracter√≠sticas**:
- Determin√≠stico: siempre genera el mismo hash para el mismo archivo
- √önico: archivos diferentes tienen hashes diferentes
- Usado para detectar duplicados

### chunk_id

**Definici√≥n**: Identificador √∫nico de un chunk basado en hash del contenido.

**Caracter√≠sticas**:
- Determin√≠stico: siempre genera el mismo hash para el mismo contenido
- √önico: chunks diferentes tienen hashes diferentes
- Usado para evitar duplicar chunks

### RPM (Requests Per Minute)

**Definici√≥n**: N√∫mero de requests (peticiones) que se pueden hacer por minuto a la API de OpenAI.

**En Tier 3**: 5,000 RPM

**Objetivo del sistema**: 3,500 RPM (70% de 5,000)

### TPM (Tokens Per Minute)

**Definici√≥n**: N√∫mero de tokens que se pueden procesar por minuto en la API de OpenAI.

**En Tier 3**: 5,000,000 TPM

**Objetivo del sistema**: 3,500,000 TPM (70% de 5,000,000)

### TPD (Tokens Per Day)

**Definici√≥n**: N√∫mero de tokens que se pueden procesar por d√≠a en la API de OpenAI.

**En Tier 3**: 100,000,000 TPD

### Tier 3

**Definici√≥n**: Nivel de acceso a la API de OpenAI con l√≠mites elevados.

**L√≠mites**:
- 5,000 RPM
- 5,000,000 TPM
- 100,000,000 TPD

**Ventajas**:
- Permite procesar grandes vol√∫menes de datos
- Ideal para ingesta masiva de documentos

---

## 11. Ideas de Mejora Futura

### 11.1. Filtros de B√∫squeda Avanzados

**Estado**: Plan futuro

**Mejoras**:
- Filtros por idioma (es, en, fr, etc.)
- Filtros por categor√≠a/tema (trading, finanzas, tecnolog√≠a, etc.)
- Filtros por autor
- Filtros por rango de a√±os
- Filtros combinados (m√∫ltiples criterios)

### 11.2. Extracci√≥n de Metadatos Autom√°tica

**Estado**: Plan futuro

**Mejoras**:
- Extraer t√≠tulo, autor, a√±o, editorial autom√°ticamente del PDF
- Clasificaci√≥n autom√°tica por categor√≠a/tema
- Detecci√≥n autom√°tica de idioma
- Extracci√≥n de resumen/abstract

### 11.3. Rerankers

**Estado**: Plan futuro

**Mejoras**:
- Usar rerankers para mejorar la precisi√≥n de la b√∫squeda
- Reordenar resultados por relevancia
- Mejorar la calidad de las respuestas

### 11.4. Evaluaci√≥n de Calidad

**Estado**: Plan futuro

**Mejoras**:
- Evaluar la calidad de las respuestas con preguntas de test
- M√©tricas de precisi√≥n, recall, F1
- Comparar diferentes configuraciones

### 11.5. UI para Explorar la Biblioteca

**Estado**: Plan futuro

**Mejoras**:
- Interfaz web para explorar documentos indexados
- B√∫squeda visual
- Visualizaci√≥n de chunks
- Estad√≠sticas de la biblioteca

### 11.6. Mejoras de Rendimiento

**Estado**: Plan futuro

**Mejoras**:
- Cache de embeddings
- Procesamiento incremental (solo archivos nuevos)
- Optimizaci√≥n de queries en pgvector
- Compresi√≥n de embeddings

### 11.7. Mejoras de Seguridad

**Estado**: Plan futuro

**Mejoras**:
- Encriptaci√≥n de documentos sensibles
- Control de acceso por usuario
- Auditor√≠a de consultas
- Logs de seguridad

---

## 12. Archivos Principales del Proyecto

### 12.1. Ingesta

- **`ingest_improved.py`**: Ingesta simple y directa
- **`ingest_optimized_rag.py`**: Ingesta optimizada con monitor y anti-duplicados
- **`config_ingesta.py`**: Configuraci√≥n centralizada de ingesta
- **`anti_duplicates.py`**: Sistema anti-duplicados
- **`ingestion_monitor.py`**: Monitor de ingesta en tiempo real

### 12.2. API y Consulta

- **`main.py`**: API FastAPI para consultas RAG
- **`config.py`**: Configuraci√≥n general del proyecto

### 12.3. Utilidades

- **`check_new_files.py`**: Verificar archivos nuevos
- **`check_status.py`**: Verificar estado de la ingesta
- **`view_data.py`**: Ver datos en Supabase
- **`check_duplicates.py`**: Verificar duplicados
- **`verificar_limites_openai.py`**: Verificar l√≠mites de OpenAI

### 12.4. Documentaci√≥n

- **`README.md`**: Este archivo
- **`PIPELINE_TECNICO.md`**: Documentaci√≥n t√©cnica del pipeline
- **`RESUMEN_ANTI_DUPLICADOS.md`**: Resumen del sistema anti-duplicados
- **`RESUMEN_MONITOR_REPORTE.md`**: Resumen del monitor y reporte

---

## 13. Conclusi√≥n

Este sistema RAG de ingesta de libros es una soluci√≥n completa y robusta para indexar y consultar grandes colecciones de documentos. Incluye:

- ‚úÖ **Ingesta masiva y paralela**
- ‚úÖ **Control de l√≠mites de API**
- ‚úÖ **Sistema anti-duplicados robusto**
- ‚úÖ **Monitor en tiempo real**
- ‚úÖ **Reporte detallado**
- ‚úÖ **API REST para consultas**
- ‚úÖ **Soporte para m√∫ltiples modelos de IA**

**Pr√≥ximos pasos**:
1. Implementar filtros de b√∫squeda avanzados
2. Mejorar extracci√≥n de metadatos
3. Agregar rerankers
4. Crear UI para explorar la biblioteca

---

## 14. Contacto y Soporte

Para preguntas o problemas, consulta la documentaci√≥n t√©cnica en los archivos `.md` del proyecto o revisa el c√≥digo fuente directamente.

**Archivos de referencia**:
- `PIPELINE_TECNICO.md`: Pipeline t√©cnico completo
- `RESUMEN_ANTI_DUPLICADOS.md`: Sistema anti-duplicados
- `RESUMEN_MONITOR_REPORTE.md`: Monitor y reporte
- `GUIA_MONITOR_REPORTE.md`: Gu√≠a de uso del monitor

---

## 15. Cambios recientes (2025-11-16)

- Escalado de Supabase al plan XL (4 cores, 16 GB RAM) para mayor concurrencia.
- Instalaci√≥n de `tenacity` en `venv_ingesta` para reintentos autom√°ticos.
- Reemplazo completo de `ingest_masiva_local.py` por versi√≥n ‚Äúanti-fracaso‚Äù:
  - Reintentos robustos con `RemoteProtocolError`, `ConnectError`, `httpx.ReadError` y `httpcore.ReadError`.
  - `upsert(..., on_conflict="doc_id")` en tabla `documents` para evitar `23505 duplicate key`.
  - Limpieza de texto eliminando `\u0000` para evitar error `22P05` en Postgres.
  - Par√°metros de alto rendimiento para XL: `MAX_WORKERS_LECTURA=20`, `DB_INSERT_BATCH_SIZE=250`, `EMBEDDING_BATCH_SIZE=256`, `CHUNK_SIZE=1000`, `CHUNK_OVERLAP=200`.
  - Correcci√≥n de import: `langchain_text_splitters.RecursiveCharacterTextSplitter`.
- Ejecuci√≥n reciente completada en ~23.42 minutos, con reintentos aplicados y progreso continuo ante fallos de red puntuales.

---

**√öltima actualizaci√≥n**: 2025-11-16
